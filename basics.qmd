# Basics {#sec-basics}

```{r child="work-in-progress.qmd"}
```

```{r components, include = FALSE}
eunit1 <- function() '<i class="fas fa-seedling"></i>'
eunit2 <- function() '<i class="fas fa-wine-bottle"></i>'
eunit3 <- function() '<i class="fas fa-laptop"></i>'
```

::: {.blockquote}
If the design of an experiment is faulty, any method of interpretation which makes it out to be decisive must be faulty too.

--Fisher, 1935, Design of Experiments
:::

There are many articles and books dedicated to explaining the concepts of experimental designs. This includes the seminal book by @Fisher1935-qc, @Bailey2008-gw, etc. This chapter presents an overview of the statistical concepts that are most pertinent in designing an experiment. 

Most experiments are comparative in nature, or more specifically, most experiments involve study of two or more experimental conditions in which the primary interest is to compare the outcome under the different conditions. You'll therefore find that most experiments in this book are **comparative experiments**. 

At the heart of each experiment, we are ultimately seeking confidence in any conclusion we are making from the analysis of the experimental data. What is perhaps not emphasised enough is that the **_experiments are human endeavours_**. Often there are multiple people involved in running experiments and a key challenge is to ensure each individual has sufficient understanding to play their role well.  We touch more on this in [Chapter -@sec-management]. 

All experiments have a cost whether that be financial, resources, time or other. We can consider the **experimental cost** as a function of the ability to redo the experiment again. For example, if your experimental resources are based on examination of fossils and the fossils are destroyed in the experimental process, then the cost of the experiment is infinite -- you only have one chance to do the experiment -- so it's absolutely essential that you plan, design and execute the experiment well.

Doing an experimental well requires a good understanding of the subject matter so that potential sources of variation can be controlled or accounted for in the experimental design. Prior to the collection of data, the statistical component of an experiment tends to be focussed on the design of the experiment, i.e. how the treatments are assigned to experimental units under practical constrains to maximise the statistical information of interest. Under this consideration, the statistical problem for experimental design is often reduced to either a randomisation or an optimisation problem and the experimental context may be stripped away in the generation of the design using computer software. We discuss this in [Chapter -@sec-context], describe a system that  encourages higher order thinking of the experimental design, termed the grammar of experimetnal designs, in [Chapter -@sec-grammar], implemented as the `edibble` R-package and present how to get started with the `edibble` system to construct experimental design in [Chapter -@sec-edibble].

Before we get into the crux of the basics of the experimental design, consider the three scenarios below. Each scenario describes an experiment where technical details have been reduced so it doesn't serve as a distraction for now. For each scenario, try to see if you can identify what are the basic components to build the design of the experiment.
 
::: {.callout-note icon=false}

## `r eunit1()` Scenario 1: plant growth  

Microbes have a potential for promoting plant growth by increasing abiotic stress tolerance of the plant. An experiment is conducted to study three bacterial strains known for promoting plant growth under osmotic stress.

:::

::: {.callout-note icon=false}

## `r eunit2()` Scenario 2: wine tasting 

A large-scale experiment was conducted to discern whether expensive wines are good value for money. The experiment involved 450 individuals in a public tasting of expensive or inexpensive wine. The taster did not know whether they were tasting the expensive or inexpensive wine. 

:::

::: {.callout-note icon=false}

## `r eunit3()` Scenario 3: new classification method 

A new statistical method is proposed as a better alternative for classification of cellular type from single cell transcriptomics data. A number of simulation, based on a number of public benchmark  datasets, was carried out to compare the method under different metrics. 

:::
 

## Basic terminology {#sec-terminology}

The field of experimental design is large and is applied to many domains. While some specialised terminology exist, the following terminology are basic terms that are commonly understood across domains:

* **treatments**: the whole set of experimental conditions in which the units are subjected to,
* **experimental unit**: the smallest unit in which the treatment can independently be applied,
* **observational unit**: the smallest unit in which the response is measured on,
* **block** or **cluster**: the grouping of another unit such that the units within the same block or cluster is more alike in some aspect than units in different blocks or clusters,
* **design**: the allocation of treatment to units, 
* **plan** or **layout**: the actual assignment of treatment to units, and
* **experimental structure**: the arrangements or constrains of experimental factors -- the structure can be broadly categorised as: 
  * **unit structure**: the relation between the units, e.g. pots nested in particular glasshouses, subjects segregated by sex, and so on; and
  * **treatment structure**: the division of treatment levels to meaningful groups, e.g. factorial structure means all combinations across the different treatment factors.
  
The above terminology describe the experimental factors and its associations. More specifically, **experimental factors** are any variables that pertain to the experiment (physical or otherwise) and includes the treatments, experimental units, observational units and blocks/clusters. The associations of experimental factors include its nesting structure or say the assignment of treatment to units. 

In addition to above, there are commonly understood terminology that refer to the properties of experimental designs. These include: 

* **balanced experiment**: refers to when the replication of the treatments are equal,
* **orthogonal design**
* **complete design**: every treatment appears exactly once in each block or cluster.

::: {.callout-note icon=false}

## `r eunit1()`  plant growth

* The experimental aim is to study the effects of plant growth of four particular bacterial strains. 
* The experimental units are ... 
:::


### Specialised terminology

Experimental design is applicable across numerous domains, so perhaps it is not surprising that there are domain specific or specialised terminology that is used to describe the whole experimental designs, e.g. Latin Square Design, Step-Wedge Trial, Beehive Design and so on. Some recipe of these named experimental design are presented and discussed in Chapter @sec-cookbook.


## Principles

All experiments have a potential to go wrong -- plans may not be followed to the letter; you can't distinguish which factor had the effect; or unknown factor is affecting the response. The degree of how wrong it went can vary but in some cases you wouldn't even know it went wrong!

In designing an experiment, there are some safety measures that you can put in place to minimise potential issues. The basic statistical principles of experimental design outlined below form some of these safety measures.  

### Randomisation and optimisation

**Randomisation** in the context of experimental design means that the treatment allocation to units were probabilistically determined by a mechanism where the outcome of the allocation is not (consciously or unconsciously) influenced by those involved in the experiment. The opposite spectrum of randomisation is **systematic** allocations of treatments to units -- this means that the allocations are not probabilistic and if you know the systematic order, then you can precisely infer the allocation.  

Randomisation is a core principle of experimental design. When you conduct an experiment, you are investigating factors to understand some process or phenomenon, and there will be factors which affect the response that you don't know about -- if you did, you could try to control that variable in your experiment. Randomisation protects the experiment, with no guarantee however, from potential bias. 

::: callout-note

## How to randomise

Typically you can use some software to draw a random sample; in this instance it'd be a good idea to ensure this randomisation can be replicated. In R, this is typically achieved by using `set.seed()`. 

In some cases, you can use physical tools to do your randomisation so long as the tool doesn't introduce some bias. E.g. flipping an unbiased coin or blindly drawing a paper with a number written from a covered box. While it's easier to replicate the randomisation using a computational tool like R, it's far more important to present the treatment allocation in a manner that the subjects involved in the experiment can accept it. For example, a water-sensitive revitalisation is planned for settlements in a developmental country such that half the settlements will be allocated to early intervention, the other half late intervention. The local community doesn't have an understanding about randomisation but the concept of lottery is familiar -- a grand ceremony is taken place with representative of the settlements drawing out a ball that allocates the settlement to early or late intervention. 

:::

Not all experiments can allocate treatments to units randomly, e.g. you can't get your participants to smoke to study the effect of smoking, nor can you get a random agricultural land to have a drought to study how well a crop grows under drought conditions. These types of experiments are referred to as *quasi-experiments* or *natural experiments*. 

**Optimisation** in the context of experimental design generally means that the treatment allocation to units were found by optimising some given criteria, usually denoted by a single letter (A, C, D, etc). These optimality-criteria are based on a statistical model, 

$$\begin{equation} \boldsymbol{y} =  \mathbf{X}\boldsymbol{\beta} + \boldsymbol{e},\end{equation}$$
where $\boldsymbol{y}$ is a vector of yet unobserved responses, $\boldsymbol{\beta}$ is a vector of experimental factor effects (e.g. treatment, block, etc), $\mathbf{X}$ is the so-called **design matrix** (comprising of 0s and 1s) that map the experimental factor levels to a response, and  $\boldsymbol{e}$ is a vector of errors. 

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"></link>


::: {.callout-note icon=false}

# `r eunit1()`  Design matrix for plant growth example

Consider again Scenario 1 where an experiment studies three bacterial strains known for promoting plant growth under osmotic stress. The design matrix, $\mathbf{X}$ below shows the mapping fo the six plant units to the baterial strain.

$$\mathbf{X} = \begin{bmatrix}1 & 0 & 0  \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1  & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 1\end{bmatrix}\qquad \boldsymbol{\beta} = \begin{bmatrix}\class{fa-solid fa-virus}{}\\ \class{fa-solid fa-bacterium}{}\\ \class{fa-solid fa-disease}{}\\ \end{bmatrix} \qquad \underset{\text{illustrative}}{\mathbf{X}\boldsymbol{\beta} =  \begin{bmatrix}\class{fa-solid fa-virus}{} \rightarrow \class{fa-solid fa-seedling}{}_1\\\class{fa-solid fa-virus}{} \rightarrow \class{fa-solid fa-seedling}{}_2\\\class{fa-solid fa-bacterium}{} \rightarrow \class{fa-solid fa-seedling}{}_3\\\class{fa-solid fa-bacterium}{} \rightarrow \class{fa-solid fa-seedling}{}_4\\\class{fa-solid fa-disease}{} \rightarrow \class{fa-solid fa-seedling}{}_5\\\class{fa-solid fa-disease}{} \rightarrow \class{fa-solid fa-seedling}{}_6\end{bmatrix}}$$


:::

If we assume $\boldsymbol{e} \sim N(\boldsymbol{0}, \sigma^2\mathbf{I})$ and $\boldsymbol{\beta}$ are fixed effects, then $\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{y}$ and $var(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}$ where $\mathbf{X}$ is assumed to be full rank. Note in particular that $var(\hat{\boldsymbol{\beta}}) \propto (\mathbf{X}^\top\mathbf{X})^{-1}$ so if the model is reasonable then the variance of parameter estimates is determined by the design matrix.

::: callout-note

## Where  $\mathbf{X}$ is not full rank

If $\mathbf{X}$ is not full rank, then there are many possible inverses of $\mathbf{X}^\top\mathbf{X}$, termed **generalised inverse** with notation as $(\mathbf{X}^\top\mathbf{X})^{-}$. 

:::

The list of optimal-criteria are:

* **A-optimality** finds $\mathbf{X}$ such that it minimises $\text{tr}\left((\mathbf{X}^\top\mathbf{X})^{-1}\right)$,
* **C-optimality** finds $\mathbf{X}$ such that it minimises $var(\boldsymbol{c}^\top\hat{\boldsymbol{\beta}})$,
* **D-optimality** finds $\mathbf{X}$ such that it maximises $det\left((\mathbf{X}^\top\mathbf{X})^{-1}\right)$,
* **E-optimality** finds $\mathbf{X}$ such that it maximises the minimume eigenvalue of $(\mathbf{X}^\top\mathbf{X})^{-1}$,
* ... and so many more 



::: callout-warning

## Optimised designs are not necessary randomised

This means that even if the design has been constructed as to optimise some criteria, you are missing out on the benefits of the randomisation. Some experimental structure results in a multiple possible treatment allocations with the same optimal criteria and the software may be selecting this in a systematic way. 

:::



::: callout-warning

## Optimal designs for complex experiments

Optimised designs for complex experiments can be computationally expensive. In these cases, the algorithm may be searching heuristically. You should try searching under different starting values and compare the optimal criteria obtained.

:::

### Bias and confounding

**Biases** can lurk into an experiment in many forms.

::: callout-note




## Examples of biases

1. To test the effectiveness of flu vaccines, a trial for vaccine 1 is done in January and a trial for vaccine 2 is done in July.
2. To study the effects of different diets for pigs, diet A is given to the first set of pigs that the farmer caught and the remaining (faster) pigs received diet B. 
3. A doctor decides to assign the treatment to the sickest patients while the control treatment is assigned to the healthiest patients. 

:::

Biases are not necessary introduced into study with malicious or known intent (e.g. trying to ensure certain treatments have a better outcome by manipulating the treatment assignment). The third example is a case where the intention of the doctor is well but is introducing a selection bias. 


### Replications, repetition and duplication

In a comparative experiment, treatments are allocated to units but every unit has an individual variation (with exceptions for some, e.g. computer experiments). To distinguish the individual variation from the treatment variation, you need **replication** of the treatment allocation to another *independent* unit. 

As a simple example, consider @fig-replication -- 2 treatments are allocated to 3 participants each.  If you ever only look at the outcome of a single replication in isolation,  you wouldn't know whether the outcome is a result of the treatment the participant received or some innate biological attribute of the participant, thus nothing to do with the treatment. Only by replicating and getting similar results with other sets of participants can you start to feel confident that the outcome may be associated with the treatment.


```{r fig-replication, echo = FALSE, fig.cap = "A comparative experiment with two treatments allocated to six participants. Each treatment is replicated thrice with the same outcome for the same treatment.", out.width="100%"}
knitr::include_graphics("figures/drawn/replications.png")
```

Suppose now that in @fig-replication, participants in Replicate 3 are biological sisters of participants in Replicate 1. In this case, Replicate 3 is not a complete replicate. As biological sisters, they would share genetics (and perhaps the environment) that makes it harder to feel confident that the outcome is due to treatment instead of other shared factors between the participants. 


#### How many replications?

#### Pseudo-replications



## Summary

The major concepts this chapter covered are:

* basic statistical terminology for experimental design 
* importance of replication and randomisation in experiments 
* pitfalls of pseudo-replication 

## Exercises

1. A farmer would like to know which diet is most effective in increasing the weight of the pigs. There are three types of diet that he wishes to test. For this scenario, describe the experimental aim, treatments, experimental units, observational units and the experimental design.
